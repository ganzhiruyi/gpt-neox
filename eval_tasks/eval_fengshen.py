"""
GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding
https://openreview.net/pdf?id=rJ4km2R5t7

The General Language Understanding Evaluation (GLUE) benchmark is a collection of
resources for training, evaluating, and analyzing natural language understanding
systems. GLUE consists of:
- A benchmark of nine sentence- or sentence-pair language understanding tasks built
on established existing datasets and selected to cover a diverse range of dataset
sizes, text genres, and degrees of difficulty, and
- A diagnostic dataset designed to evaluate and analyze model performance with
respect to a wide range of linguistic phenomena found in natural language.

Homepage: https://gluebenchmark.com/
"""
import numpy as np
from lm_eval.base import rf, Task
from lm_eval.metrics import mean, matthews_corrcoef, f1_score, yesno
from lm_eval.utils import general_detokenize
from lm_eval import metrics
from rouge import Rouge

rouger = Rouge()


def rouge_l(items):
    hyps, refs = map(list, zip(*[[d[0], d[1]] for d in items]))
    scores = rouger.get_scores(hyps, refs, avg=True)
    return scores


class GenerateTask(Task):
    VERSION = 0
    DATASET_PATH = None
    DATASET_NAME = None

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset['train']

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):
        return doc["input"]

    def should_decontaminate(self):
        return False

    def doc_to_decontamination_query(self, doc):
        return doc["question"]

    def doc_to_target(self, doc):
        return '\n\n' + doc['output']

    def construct_requests(self, doc, ctx):
        continuation = rf.greedy_until(ctx, ["<|endoftext|>"])
        return continuation

    def process_results(self, doc, results):
        ref_pred = (doc["output"], results)
        ref_pred_rouge = (doc["output"], results[0] +
                          'ã€‚') if results[0] == '' else (doc["output"], results[0])
        return {
            "bleu": ref_pred,
            "rouge_l": ref_pred_rouge,
            "chrf": ref_pred,
            "ter": ref_pred,
        }

    def aggregation(self):
        """
        :returns: {str: [float] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metrics
        """
        return {
            "bleu": metrics.bleu,
            "rouge_l": rouge_l,
            "chrf": metrics.chrf,
            "ter": metrics.ter,
        }

    def higher_is_better(self):
        """
        :returns: {str: bool}
            A dictionary where keys are the names of submetrics and values are
            whether a higher value of the submetric is better
        """
        return {
            "bleu": True,
            "rouge_l": True,
            "chrf": True,
            "ter": False,
        }


class ClassificationTask(Task):
    VERSION = 0
    DATASET_PATH = None
    DATASET_NAME = None

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset['train']

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):

        return doc['input']

    def should_decontaminate(self):
        return False

    def doc_to_decontamination_query(self, doc):
        return doc["premise"]

    def doc_to_target(self, doc):
        return "\n\n" + [doc["output"]]

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        choice_scores = []
        for c in doc['choice']:
            score, _ = rf.loglikelihood(ctx, "\n\n"+c)
            choice_scores.append(score)
        return choice_scores

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        gold = doc["label"]
        pred = np.argmax(results)
        return {"acc": pred == gold}

    def aggregation(self):
        """
        :returns: {str: [float] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metrics
        """
        return {"acc": mean}

    def higher_is_better(self):
        """
        :returns: {str: bool}
            A dictionary where keys are the names of submetrics and values are
            whether a higher value of the submetric is better
        """
        return {"acc": True}


class webqa(GenerateTask):
    DATASET_PATH = 'suolyer/webqa'


class cmqa(GenerateTask):
    DATASET_PATH = 'suolyer/cmqa'


class lcsts(GenerateTask):
    DATASET_PATH = 'suolyer/lcsts'


class translate_en2zh(GenerateTask):
    DATASET_PATH = 'suolyer/translate_en2zh'


class translate_zh2en(GenerateTask):
    DATASET_PATH = 'suolyer/translate_zh2en'


class c3(ClassificationTask):
    DATASET_PATH = 'suolyer/c3'


class ocnli(ClassificationTask):
    DATASET_PATH = 'suolyer/ocnli'


TASK_REGISTRY = {
    "webqa": webqa,
    "cmqa": cmqa,
    "translate_en2zh": translate_en2zh,
    "translate_zh2en": translate_zh2en,
    "lcsts": lcsts,
    "c3": c3,
    "ocnli": ocnli,
}

all_tasks_zh = [k for k in TASK_REGISTRY.keys()]


def get_task(task_name):
    try:
        return TASK_REGISTRY[task_name]
    except KeyError:
        print("Available tasks:")
        pprint(TASK_REGISTRY)
        raise KeyError(f"Missing task {task_name}")


def get_task_dict_zh(task_name_list):
    task_name_dict = {
        task_name: get_task(task_name)()
        for task_name in task_name_list
        if isinstance(task_name, str)
    }
    return task_name_dict
